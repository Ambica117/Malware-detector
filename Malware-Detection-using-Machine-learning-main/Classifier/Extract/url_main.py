import pandas as pd
import numpy as np
import random
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import pickle

# Sanitization function for URL tokenization
def sanitization(web):
    web = web.lower()
    token = []
    dot_token_slash = []
    raw_slash = str(web).split('/')
    for i in raw_slash:
        raw1 = str(i).split('-')
        slash_token = []
        for j in range(0, len(raw1)):
            raw2 = str(raw1[j]).split('.')
            slash_token = slash_token + raw2
        dot_token_slash = dot_token_slash + raw1 + slash_token
    token = list(set(dot_token_slash)) 
    if 'com' in token:
        token.remove('com')
    return token

# Input URL for checking
urls = []
urls.append(input("Input the URL that you want to check (e.g., google.com): "))

# Whitelist of known good domains
whitelist = ['hackthebox.eu', 'root-me.org', 'gmail.com']
s_url = [i for i in urls if i not in whitelist]

# Load the model
with open("pickel_model.pkl", 'rb') as f1:  
    lgr = pickle.load(f1)

# Load the vectorizer
with open("pickel_vector.pkl", 'rb') as f2:  
    vectorizer = pickle.load(f2)

# Ensure the vectorizer and model are fitted
if hasattr(vectorizer, 'vocabulary_') and hasattr(lgr, 'coef_'):
    x = vectorizer.transform(s_url)
    y_predict = lgr.predict(x)
else:
    raise ValueError("The loaded vectorizer or model is not properly fitted.")

# Append whitelist domains to results
for site in whitelist:
    s_url.append(site)

# Print prediction results
predict = list(y_predict)
for j in range(0, len(whitelist)):
    predict.append('good')
print("\nThe entered domain is: ", predict[0])
print("\nIf you feel that this prediction is wrong or if you are not sure about this output,\nyou can contact us at kabirdhruw@protonmail.com. We'll check the URL and update the machine accordingly. Thank you.")
